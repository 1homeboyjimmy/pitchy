
# Документация по системе RAG (Retrieval-Augmented Generation)

## 1. Что такое RAG?
RAG (Retrieval-Augmented Generation) — это метод, который позволяет языковой модели (LLM) отвечать на вопросы, используя ваши собственные данные, которых не было в её обучающей выборке.

**Принцип работы:**
1.  Пользователь задает вопрос.
2.  Система ищет (Retrieval) релевантную информацию в вашей базе знаний.
3.  Система передает (Augmentation) найденную информацию вместе с вопросом в LLM.
4.  LLM генерирует (Generation) ответ на основе этих данных.

---

## 2. Текущая реализация (Semantic Search)

Мы обновили ядро системы с простого поиска по ключевым словам (TF-IDF) на **Семантический поиск**.

### Почему это важно?
*   **TF-IDF (было):** Искал точные совпадения слов. Если спросить "Где взять денег?", а в тексте написано "Привлечение инвестиций", система могла не найти ответ, так как слова разные.
*   **Семантический поиск (стало):** Превращает текст в векторы (числа), отражающие смысл. "Деньги" и "Инвестиции" находятся близко в векторном пространстве, поэтому система понимает контекст.

---

## 3. Компоненты системы

### А. Сбор данных (`scraper.py`)
Этот скрипт отвечает за наполнение базы знаний.
*   **Функция:** Скачивает веб-страницы по списку URL.
*   **Особенность:** Обходит SSL-проверки и очищает HTML от мусора (скриптов, стилей), оставляя только полезный текст.
*   **Результат:** Сохраняет чистые текстовые файлы в папку `sample_docs/`.

### Б. Ядро RAG (`rag.py`)
Основной модуль, который управляет всем процессом индексации и поиска.

1.  **Загрузка (Loading):** Считывает все `.txt` файлы из `sample_docs/`.
2.  **Чанкинг (Chunking):**
    *   Использует `RecursiveCharacterTextSplitter`.
    *   Разбивает длинные тексты на небольшие "чанки" (по ~1000 символов) с перекрытием. Это нужно, чтобы не терять контекст на границах и чтобы векторы были точнее.
3.  **Эмбеддинг (Embedding):**
    *   Использует модель **`cointegrated/rubert-tiny2`**.
    *   Это быстрая и легкая модель, оптимизированная для русского языка.
    *   **Локальный запуск:** Модель загружается из папки `model_data/rubert-tiny2`, а не из интернета. Это гарантирует работу сервера даже без доступа к HuggingFace.
4.  **Векторная база (Vector DB):**
    *   Использует **ChromaDB**.
    *   Хранит векторы документов и позволяет производить быстрый поиск ближайших соседей (Nearest Neighbor Search).

---

## 4. Файловая структура

*   `rag.py` — основной код класса `StartupRAG`.
*   `scraper.py` — скрипт для скачивания новых данных.
*   `download_model.py` — сервисный скрипт для скачивания весов модели (запускается один раз при развертывании).
*   `test_rag.py` — скрипт для проверки работоспособности поиска.
*   `model_data/` — папка конфигурацией модели `rubert-tiny2` (бинарные веса `pytorch_model.bin` скачиваются отдельно).
*   `sample_docs/` — папка с исходными текстовыми документами.
*   `chroma_db/` — папка, где ChromaDB хранит свои индексы (создается автоматически).

---

## 5. Как использовать

### Добавление новых знаний
1.  Добавьте ссылки в список `urls` в `scraper.py` или просто положите `.txt` файлы в `sample_docs/`.
2.  Запустите скрепер:
    ```bash
    python scraper.py
    ```
3.  Перезапустите приложение (или скрипт RAG), чтобы пересоздать индекс. При старте `rag.py` автоматически переиндексирует документы.

### Проверка поиска
Чтобы убедиться, что система находит информацию:
```bash
python test_rag.py
```
Скрипт сделает пробный запрос ("стартап") и покажет найденные фрагменты текста.

---

## 6. Развертывание на сервере

Папка `model_data` содержит конфигурацию, но **большой файл весов (`pytorch_model.bin`) не включен в репозиторий** из-за ограничений GitHub.

Для запуска на сервере:
1.  Сделайте `git pull`.
2.  Установите зависимости:
    ```bash
    pip install -r requirements.txt
    ```
3.  **Обязательно запустите загрузчик модели:**
    ```bash
    python download_model.py
    ```
    Этот скрипт скачает недостающий файл `pytorch_model.bin` (~112 MB) и доукомплектует модель.
4.  Всё готово! Приложение будет использовать локальную модель.
